{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-23T00:23:32.934390Z",
     "start_time": "2024-12-23T00:23:32.930132Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from kaggle_nlp.utils.utilities import PreprocessingUtils"
   ],
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:15:12.262132Z",
     "start_time": "2024-12-23T00:15:12.259703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize utils object\n",
    "utils = PreprocessingUtils()"
   ],
   "id": "345baea474e3a6b6",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tweets_df = pd.read_csv('data/train.csv')",
   "id": "ac1105d621bb5035"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data Preprocessing",
   "id": "670e157bcd96a03b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:17:38.761870Z",
     "start_time": "2024-12-23T00:17:38.756201Z"
    }
   },
   "cell_type": "code",
   "source": "tweets_df.head()",
   "id": "e5e1b83f84f45055",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:17:39.329605Z",
     "start_time": "2024-12-23T00:17:39.323529Z"
    }
   },
   "cell_type": "code",
   "source": "tweets_df.info()",
   "id": "6b90c5962967ca9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:17:39.756488Z",
     "start_time": "2024-12-23T00:17:39.752230Z"
    }
   },
   "cell_type": "code",
   "source": "tweets_df['target'].value_counts()",
   "id": "2d8d9c29ae07ddbc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:17:40.454063Z",
     "start_time": "2024-12-23T00:17:40.446697Z"
    }
   },
   "cell_type": "code",
   "source": "tweets_clean = tweets_df.copy()",
   "id": "41ff3c021426351c",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:17:41.160049Z",
     "start_time": "2024-12-23T00:17:41.154460Z"
    }
   },
   "cell_type": "code",
   "source": "tweets_clean['keyword'].value_counts()",
   "id": "7a926440190aa03e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keyword\n",
       "fatalities               45\n",
       "deluge                   42\n",
       "armageddon               42\n",
       "sinking                  41\n",
       "damage                   41\n",
       "                         ..\n",
       "forest%20fire            19\n",
       "epicentre                12\n",
       "threat                   11\n",
       "inundation               10\n",
       "radiation%20emergency     9\n",
       "Name: count, Length: 221, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:17:41.935784Z",
     "start_time": "2024-12-23T00:17:41.932024Z"
    }
   },
   "cell_type": "code",
   "source": "tweets_clean = utils.clean_keywords(tweets_clean)",
   "id": "54d34e5d01d8fb4b",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:17:42.503522Z",
     "start_time": "2024-12-23T00:17:42.497579Z"
    }
   },
   "cell_type": "code",
   "source": "tweets_clean['combined_text'] = tweets_clean.apply(utils.combine_text, axis=1)",
   "id": "9d8403a3cef75d60",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:17:43.092424Z",
     "start_time": "2024-12-23T00:17:43.090254Z"
    }
   },
   "cell_type": "code",
   "source": "tweets_clean['combined_text'] = tweets_clean['combined_text'].apply(utils.clean_text)",
   "id": "d2ddc7f818a4c91",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def select_columns(tweets):\n",
    "    tweets = tweets_clean.drop(columns=['keyword', 'location', 'text'])\n",
    "    return tweets\n",
    "\n",
    "df = select_columns(tweets_clean)\n",
    "df.head()"
   ],
   "id": "354c6aaaef702d9b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define a Dataset Class",
   "id": "e41ef76cbe492f15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DisasterTweetsDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        target = self.targets[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(target, dtype=torch.long),\n",
    "        }"
   ],
   "id": "3522763179df70c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load and Tokenize Data",
   "id": "16b2aa6c48de5cea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_texts, test_texts, train_targets, test_targets = train_test_split(\n",
    "    df['combined_text'].tolist(), df['target'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = DisasterTweetsDataset(train_texts, train_targets, tokenizer, max_len=128)\n",
    "test_dataset = DisasterTweetsDataset(test_texts, test_targets, tokenizer, max_len=128)"
   ],
   "id": "81ab9d2043666e82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define a DataLoader",
   "id": "ae5dc09e90601cd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ],
   "id": "ca1eb9937f29909c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define the Model",
   "id": "e2af524825bb226a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)",
   "id": "e54ea0e579a7cd96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training",
   "id": "66bb929744dca882"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch + 1}/{epochs} completed')"
   ],
   "id": "d7e927c14c682316"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluation",
   "id": "af3554206af2221e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T23:16:03.721492Z",
     "start_time": "2024-12-22T23:16:03.313760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy}')"
   ],
   "id": "512f1caf47819f8d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/tokenizer_config.json',\n",
       " 'model/special_tokens_map.json',\n",
       " 'model/vocab.txt',\n",
       " 'model/added_tokens.json',\n",
       " 'model/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T00:29:58.800973Z",
     "start_time": "2024-12-23T00:29:58.443446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the trained model and the tokenizer\n",
    "model_dir = 'model'\n",
    "\n",
    "try:\n",
    "    os.makedirs(f'{os.getcwd()}/{model_dir}')\n",
    "    print(f'Directory {model_dir} created')\n",
    "except FileExistsError:\n",
    "    print(f'Directory {model_dir} already exists')\n",
    "finally:\n",
    "    model.save_pretrained(model_dir)\n",
    "    tokenizer.save_pretrained(model_dir)\n",
    "    print(f'Model and tokenizer saved to directory{model_dir}')"
   ],
   "id": "58670d5625712adc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory model already exists\n",
      "Model and tokenizer saved to directorymodel\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "911dd3e6844ee16b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "15204c9c532bbaae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
